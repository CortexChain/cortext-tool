version: '3.8'
services:
  # Kafka service with KRaft and SASL_PLAINTEXT
  kafka:
    image: bitnami/kafka:3.7.0
    container_name: contextchain-kafka
    environment:
      # KRaft configuration
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@contextchain-kafka:9094
      # Mixed listener configuration - both PLAINTEXT and SASL
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,SASL_PLAINTEXT://:9093,CONTROLLER://:9094,EXTERNAL://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,SASL_PLAINTEXT://kafka:9093,EXTERNAL://localhost:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT,CONTROLLER:PLAINTEXT,EXTERNAL:SASL_PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_BROKER_ID=1
      - ALLOW_PLAINTEXT_LISTENER=yes
      # SASL configuration
      - KAFKA_CFG_SASL_ENABLED_MECHANISMS=PLAIN
      - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=PLAIN
      - KAFKA_CLIENT_USERS=admin
      - KAFKA_CLIENT_PASSWORDS=123456A@a
      # Storage & Log optimization
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824  # 1GB segment size
      - KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES=10000  # Flush frequency
      - KAFKA_CFG_LOG_FLUSH_INTERVAL_MS=1000
      - KAFKA_CFG_COMPRESSION_TYPE=lz4  # LZ4 compression
    volumes:
      - kafka_data:/bitnami/kafka
      - ./kafka/client.properties:/opt/bitnami/kafka/config/client.properties:ro
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '2.0'

  # Redis Master for Write Operations
  redis-master:
    image: redis:7.2-alpine
    container_name: contextchain-redis-master
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis/redis-master.conf:/usr/local/etc/redis/redis.conf:ro
      - redis_master_data:/data
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD", "redis-cli", "-n", "0", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # Redis Slave for Read Operations
  redis-slave:
    image: redis:7.2-alpine
    container_name: contextchain-redis-slave
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis/redis-slave.conf:/usr/local/etc/redis/redis.conf:ro
      - redis_slave_data:/data
    depends_on:
      redis-master:
        condition: service_healthy
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD", "redis-cli", "-n", "0", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'

  # Nginx Load Balancer and Reverse Proxy
  nginx:
    image: nginx:1.25-alpine
    container_name: contextchain-nginx
    ports:
      - "80:80"        # HTTP Main
      - "443:443"      # HTTPS (future SSL)
      - "8080:8080"    # AKHQ proxy
      - "5601:5601"    # Kibana proxy
      - "9200:9200"    # Elasticsearch proxy
      - "9308:9308"    # Kafka Exporter proxy
      - "6379:6379"    # Redis Write (Master only)
      - "6380:6380"    # Redis Read (Slave only)
      - "9092:9092"    # Kafka PLAINTEXT proxy
      - "9093:9093"    # Kafka SASL proxy
      - "29092:29092"  # Kafka External proxy
      - "9999:9999"    # Kafka JMX proxy
      - "5044:5044"    # Logstash Beats proxy
      - "5001:5001"    # Logstash TCP/UDP proxy
      - "9600:9600"    # Logstash API proxy
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - akhq
      - kibana
      - elasticsearch
      - kafka-exporter
      - redis-master
      - redis-slave
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # AKHQ with SASL authentication
  akhq:
    container_name: contextchain-akhq
    image: tchiotludo/akhq:0.23.0
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          connections:
            local:
              properties:
                bootstrap.servers: "contextchain-kafka:9093"
                security.protocol: SASL_PLAINTEXT
                sasl.mechanism: PLAIN
                sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="123456A@a";
              jmx:
                port: 9999
          security:
            enabled: false
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - contextchain-net

  # Kafka Exporter for Prometheus metrics with SASL
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: contextchain-kafka-exporter
    command:
      - --kafka.server=contextchain-kafka:9093
      - --web.listen-address=:9308
      - --sasl.enabled
      - --sasl.mechanism=PLAIN
      - --sasl.username=admin
      - --sasl.password=123456A@a
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - contextchain-net

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: contextchain-elasticsearch
    environment:
      - node.name=elasticsearch
      - cluster.name=contextchain-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: contextchain-logstash
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
    environment:
      LS_JAVA_OPTS: "-Xmx1g -Xms1g"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - contextchain-net

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: contextchain-kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://contextchain-elasticsearch:9200
      - SERVER_NAME=contextchain-kibana
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Filebeat for log collection
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: contextchain-filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat_data:/usr/share/filebeat/data
    depends_on:
      elasticsearch:
        condition: service_healthy
      logstash:
        condition: service_started
    networks:
      - contextchain-net
    command: filebeat -e -strict.perms=false

networks:
  contextchain-net:
    driver: bridge

volumes:
  kafka_data:
    driver: local
  elasticsearch_data:
    driver: local
  filebeat_data:
    driver: local
  redis_master_data:
    driver: local
  redis_slave_data:
    driver: local
  nginx_logs:
    driver: local