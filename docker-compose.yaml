version: '3.8'
services:  
  # Zookeeper service
  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: contextchain-zookeeper
    ports:
      - "2181:2181"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
      - ZOO_SERVER_ID=1
      - ZOO_PORT_NUMBER=2181
      # JVM Performance tuning với G1GC
      - JVMFLAGS=-Xms512m -Xmx1g -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35
    volumes:
      - zookeeper_data:/bitnami/zookeeper
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD", "nc", "-vz", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1.5G
          cpus: '1.0'

  # Kafka với storage & log optimization + JMX monitoring
  kafka:
    image: bitnami/kafka:latest
    container_name: contextchain-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"  # JMX port for monitoring
    environment:
      # Basic Kafka config
      - KAFKA_CFG_BROKER_ID=1
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,EXTERNAL://:29092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:29092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_ZOOKEEPER_CONNECT=contextchain-zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      
      # Storage & Log optimization
      - KAFKA_CFG_LOG_SEGMENT_BYTES=1073741824  # 1GB segment size
      - KAFKA_CFG_LOG_FLUSH_INTERVAL_MESSAGES=10000  # Flush frequency
      - KAFKA_CFG_LOG_FLUSH_INTERVAL_MS=1000
      - KAFKA_CFG_COMPRESSION_TYPE=lz4  # LZ4 compression
        
    volumes:
      - kafka_data:/bitnami/kafka
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '2.0'
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "10"

  # Kafka UI
  kafka-ui:
    container_name: contextchain-kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    environment:
      DYNAMIC_CONFIG_ENABLED: true
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: contextchain-kafka:9092
      KAFKA_CLUSTERS_0_METRICS_PORT: 9999
      KAFKA_CLUSTERS_0_JMXPORT: 9999
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - contextchain-net

  # Kafka Exporter cho Prometheus metrics
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: contextchain-kafka-exporter
    command:
      - --kafka.server=contextchain-kafka:9092
      - --web.listen-address=:9308
    ports:
      - "9308:9308"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - contextchain-net

  # ================================
  # ELK STACK
  # ================================
  
  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: contextchain-elasticsearch
    environment:
      - node.name=elasticsearch
      - cluster.name=contextchain-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: contextchain-logstash
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
    ports:
      - "5044:5044"
      - "5001:5001/tcp"
      - "5001:5001/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx1g -Xms1g"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - contextchain-net

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: contextchain-kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://contextchain-elasticsearch:9200
      - SERVER_NAME=contextchain-kibana
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - contextchain-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Filebeat để collect logs
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.0
    container_name: contextchain-filebeat
    user: root
    volumes:
      - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - filebeat_data:/usr/share/filebeat/data
    depends_on:
      elasticsearch:
        condition: service_healthy
      logstash:
        condition: service_started
    networks:
      - contextchain-net
    command: filebeat -e -strict.perms=false

networks:
  contextchain-net:
    driver: bridge

volumes:
  kafka_data:
    driver: local
  zookeeper_data:
    driver: local
  elasticsearch_data:
    driver: local
  filebeat_data:
    driver: local